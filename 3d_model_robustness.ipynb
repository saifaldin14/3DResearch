{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAbstractionLayer(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp):\n",
    "        super(SetAbstractionLayer, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        \n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        \n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, N, 3]\n",
    "            points: input points data, [B, N, D]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, S, 3]\n",
    "            new_points_concat: sample points feature data, [B, S, D']\n",
    "        \"\"\"\n",
    "        # Simplified implementation - in a real model this would use farthest point sampling\n",
    "        # and ball query operations from PointNet++ \n",
    "        B, N, C = xyz.shape\n",
    "        if self.npoint is None:\n",
    "            S = N\n",
    "        else:\n",
    "            S = min(self.npoint, N)\n",
    "            \n",
    "        # Simplified: just use the first S points instead of FPS\n",
    "        new_xyz = xyz[:, :S, :]\n",
    "        \n",
    "        # Simplified feature aggregation (in real implementation would be grouped by radius)\n",
    "        if points is not None:\n",
    "            new_points = points[:, :S, :]\n",
    "        else:\n",
    "            new_points = new_xyz\n",
    "            \n",
    "        # Apply MLP - simplified for this example\n",
    "        new_points = new_points.permute(0, 2, 1).unsqueeze(-1)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points = torch.relu(bn(conv(new_points)))\n",
    "        \n",
    "        new_points = new_points.squeeze(-1).permute(0, 2, 1)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "class PointNet2Classification(nn.Module):\n",
    "    def __init__(self, num_classes=40, normal_channel=False):\n",
    "        super(PointNet2Classification, self).__init__()\n",
    "        in_channel = 6 if normal_channel else 3\n",
    "        \n",
    "        self.normal_channel = normal_channel\n",
    "        \n",
    "        # SA modules\n",
    "        self.sa1 = SetAbstractionLayer(npoint=512, radius=0.2, nsample=32, \n",
    "                                        in_channel=in_channel, mlp=[64, 64, 128])\n",
    "        self.sa2 = SetAbstractionLayer(npoint=128, radius=0.4, nsample=64, \n",
    "                                        in_channel=128 + 3, mlp=[128, 128, 256])\n",
    "        self.sa3 = SetAbstractionLayer(npoint=None, radius=None, nsample=None, \n",
    "                                        in_channel=256 + 3, mlp=[256, 512, 1024])\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        B, _, _ = xyz.shape\n",
    "        if self.normal_channel:\n",
    "            norm = xyz[:, 3:, :]\n",
    "            xyz = xyz[:, :3, :]\n",
    "        else:\n",
    "            norm = None\n",
    "        \n",
    "        # Transpose to match expected input format\n",
    "        xyz = xyz.transpose(2, 1)\n",
    "        \n",
    "        # Set Abstraction layers\n",
    "        l1_xyz, l1_points = self.sa1(xyz, norm)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        \n",
    "        # FC layers\n",
    "        x = l3_points.view(B, 1024)\n",
    "        x = self.drop1(torch.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(torch.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def _init_pointnet_plus_plus(num_classes=40):\n",
    "    \"\"\"Create and initialize a PointNet++ classification model\"\"\"\n",
    "    model = PointNet2Classification(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv(nn.Module):\n",
    "    def __init__(self, k, in_channels, out_channels):\n",
    "        super(EdgeConv, self).__init__()\n",
    "        self.k = k\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*2, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        \n",
    "    def knn(self, x, k):\n",
    "        inner = -2 * torch.matmul(x.transpose(2, 1), x)\n",
    "        xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "        distance = -xx - inner - xx.transpose(2, 1)\n",
    "        \n",
    "        idx = distance.topk(k=k, dim=-1)[1]\n",
    "        return idx\n",
    "    \n",
    "    def get_graph_feature(self, x, k=20):\n",
    "        batch_size, num_dims, num_points = x.size()\n",
    "        idx = self.knn(x, k)\n",
    "        idx_base = torch.arange(0, batch_size, device=x.device).view(-1, 1, 1) * num_points\n",
    "        idx = idx + idx_base\n",
    "        idx = idx.view(-1)\n",
    "        \n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        feature = x.view(batch_size * num_points, -1)[idx, :]\n",
    "        feature = feature.view(batch_size, num_points, k, num_dims)\n",
    "        x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "        \n",
    "        feature = torch.cat((feature - x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "        return feature\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.get_graph_feature(x, self.k)\n",
    "        x = self.conv(x)\n",
    "        x = x.max(dim=-1, keepdim=False)[0]\n",
    "        return x\n",
    "\n",
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, num_classes=40, k=20):\n",
    "        super(DGCNN, self).__init__()\n",
    "        self.k = k\n",
    "        \n",
    "        # Edge convolution layers\n",
    "        self.edge_conv1 = EdgeConv(k=k, in_channels=3, out_channels=64)\n",
    "        self.edge_conv2 = EdgeConv(k=k, in_channels=64, out_channels=64)\n",
    "        self.edge_conv3 = EdgeConv(k=k, in_channels=64, out_channels=128)\n",
    "        self.edge_conv4 = EdgeConv(k=k, in_channels=128, out_channels=256)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv1d(512, 1024, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input transform\n",
    "        batch_size = x.size(0)\n",
    "        if x.shape[1] == 3 and len(x.shape) == 3:  # BxCxN format\n",
    "            pass\n",
    "        else:  # Assuming BxNxC format (like PointNet)\n",
    "            x = x.transpose(2, 1)\n",
    "            \n",
    "        # Extract edge features\n",
    "        x1 = self.edge_conv1(x)\n",
    "        x2 = self.edge_conv2(x1)\n",
    "        x3 = self.edge_conv3(x2)\n",
    "        x4 = self.edge_conv4(x3)\n",
    "        \n",
    "        # Concatenate features\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        \n",
    "        # MLP\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # Global max pooling\n",
    "        x = torch.max(x, 2)[0]\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.drop1(torch.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(torch.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def _init_dgcnn(num_classes=40):\n",
    "    \"\"\"Create and initialize a DGCNN classification model\"\"\"\n",
    "    model = DGCNN(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointMLP(nn.Module):\n",
    "    def __init__(self, num_classes=40, points=1024, embed_dim=64):\n",
    "        super(PointMLP, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.points = points\n",
    "        \n",
    "        # Point embedding layers\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(3, embed_dim, 1),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(embed_dim, embed_dim, 1),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Hierarchical feature extraction layers\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512, 512, 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle input format (B, N, 3) or (B, 3, N)\n",
    "        batch_size = x.size(0)\n",
    "        if x.shape[1] != 3 and x.shape[2] == 3:\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        # Feature extraction\n",
    "        x = self.embedding(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = torch.max(x, 2)[0]\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def _init_pointmlp(num_classes=40):\n",
    "    \"\"\"Create and initialize a PointMLP classification model\"\"\"\n",
    "    model = PointMLP(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointAttentionNet(nn.Module):\n",
    "    def __init__(self, num_classes=40, num_points=1024, embed_dim=128):\n",
    "        super(PointAttentionNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Point embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(3, embed_dim, 1),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Self-attention modules\n",
    "        self.self_attn1 = PointSelfAttention(embed_dim, 4)\n",
    "        self.self_attn2 = PointSelfAttention(embed_dim, 4)\n",
    "        \n",
    "        # Feature processing after attention\n",
    "        self.feature_conv = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, 512, 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle input format (B, N, 3) or (B, 3, N)\n",
    "        batch_size = x.size(0)\n",
    "        if x.shape[1] != 3 and x.shape[2] == 3:\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        # Feature embedding\n",
    "        x = self.embedding(x)  # B x embed_dim x N\n",
    "        \n",
    "        # Apply attention\n",
    "        x = self.self_attn1(x)\n",
    "        x = self.self_attn2(x)\n",
    "        \n",
    "        # Feature processing\n",
    "        x = self.feature_conv(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = torch.max(x, 2)[0]\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PointSelfAttention(nn.Module):\n",
    "    def __init__(self, channels, heads=4):\n",
    "        super(PointSelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.channels = channels\n",
    "        self.head_dim = channels // heads\n",
    "        assert self.head_dim * heads == channels, \"channels must be divisible by heads\"\n",
    "        \n",
    "        self.qkv_conv = nn.Conv1d(channels, channels * 3, 1, bias=False)\n",
    "        self.out_conv = nn.Conv1d(channels, channels, 1)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm1d(channels)\n",
    "        self.norm2 = nn.BatchNorm1d(channels)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels * 2, 1),\n",
    "            nn.BatchNorm1d(channels * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels * 2, channels, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: B x C x N\n",
    "        batch_size, C, N = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Self-attention\n",
    "        qkv = self.qkv_conv(x)  # B x 3C x N\n",
    "        qkv = qkv.reshape(batch_size, 3, self.heads, self.head_dim, N)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]  # B x H x D x N\n",
    "        \n",
    "        # Compute attention scores\n",
    "        q = q.permute(0, 1, 3, 2)  # B x H x N x D\n",
    "        k = k.permute(0, 1, 2, 3)  # B x H x D x N\n",
    "        attn = torch.matmul(q, k) / (self.head_dim ** 0.5)  # B x H x N x N\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        v = v.permute(0, 1, 3, 2)  # B x H x N x D\n",
    "        x = torch.matmul(attn, v)  # B x H x N x D\n",
    "        x = x.permute(0, 1, 3, 2).reshape(batch_size, C, N)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.out_conv(x)\n",
    "        x = self.norm1(x + residual)\n",
    "        \n",
    "        # Feed forward\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm2(x + residual)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_custom_attention(num_classes=40):\n",
    "    \"\"\"Create and initialize a custom attention-based model\"\"\"\n",
    "    return PointAttentionNet(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import model architectures\n",
    "# Note: These would typically be imported from external files\n",
    "# For demonstration, we'll include placeholders for imports\n",
    "\n",
    "# from models.pointnet2 import PointNet2Classification\n",
    "# from models.dgcnn import DGCNN\n",
    "# from models.pointmlp import PointMLP\n",
    "# from models.custom_attention import PointAttentionNet\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model_name, pretrained_path, num_classes=40):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initialize selected model\n",
    "        if model_name == \"pointnet++\":\n",
    "            self.model = self._init_pointnet_plus_plus()\n",
    "        elif model_name == \"dgcnn\":\n",
    "            self.model = self._init_dgcnn()\n",
    "        elif model_name == \"pointmlp\":\n",
    "            self.model = self._init_pointmlp()\n",
    "        elif model_name == \"custom\":\n",
    "            self.model = self._init_custom_attention()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "            \n",
    "        # Load pre-trained weights\n",
    "        self.load_pretrained(pretrained_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def _init_pointnet_plus_plus(self):\n",
    "        return PointNet2Classification(num_classes=self.num_classes)\n",
    "        \n",
    "    def _init_dgcnn(self):\n",
    "        # Placeholder for DGCNN initialization\n",
    "        return DGCNN(num_classes=self.num_classes)\n",
    "        \n",
    "    def _init_pointmlp(self):\n",
    "        # Placeholder for PointMLP initialization\n",
    "        return PointMLP(num_classes=self.num_classes)\n",
    "        \n",
    "    def _init_custom_attention(self):\n",
    "        # Placeholder for custom attention model initialization\n",
    "        # return PointAttentionNet(num_classes=self.num_classes)\n",
    "        print(\"Initializing custom attention model\")\n",
    "        return None  # Replace with actual model\n",
    "    \n",
    "    def load_pretrained(self, path):\n",
    "        # Load pre-trained weights if the model exists\n",
    "        if self.model is not None and os.path.exists(path):\n",
    "            print(f\"Loading pre-trained weights for {self.model_name} from {path}\")\n",
    "            # self.model.load_state_dict(torch.load(path))\n",
    "        else:\n",
    "            print(f\"No pre-trained weights found at {path}\")\n",
    "    \n",
    "    def evaluate_adversarial_robustness(self, test_loader, attack_type, epsilon):\n",
    "        \"\"\"\n",
    "        Evaluate model robustness against adversarial attacks\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_loader: DataLoader containing test data\n",
    "        attack_type: str, type of attack (e.g., 'fgsm', 'pgd')\n",
    "        epsilon: float, attack strength\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy: float, model accuracy under adversarial attack\n",
    "        \"\"\"\n",
    "        # Placeholder for adversarial evaluation\n",
    "        print(f\"Evaluating {self.model_name} against {attack_type} with ε={epsilon}\")\n",
    "        return 0.0\n",
    "    \n",
    "    def evaluate_corruption_robustness(self, test_loader, corruption_type, severity):\n",
    "        \"\"\"\n",
    "        Evaluate model robustness against environmental corruptions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_loader: DataLoader containing test data\n",
    "        corruption_type: str, type of corruption (e.g., 'gaussian', 'impulse')\n",
    "        severity: int, corruption severity level (usually 1-5)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy: float, model accuracy under corruption\n",
    "        \"\"\"\n",
    "        # Placeholder for corruption evaluation\n",
    "        print(f\"Evaluating {self.model_name} against {corruption_type} corruption at severity {severity}\")\n",
    "        return 0.0\n",
    "\n",
    "# Example usage:\n",
    "# evaluator = ModelEvaluator(\"pointnet++\", \"pretrained/pointnet_modelnet40.pth\")\n",
    "# adv_acc = evaluator.evaluate_adversarial_robustness(test_loader, \"pgd\", 0.1)\n",
    "# corr_acc = evaluator.evaluate_corruption_robustness(test_loader, \"gaussian_noise\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import random\n",
    "from scipy.spatial.transform import Rotation\n",
    "import math\n",
    "\n",
    "class ModelNet40Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', num_points=1024, transform=None, random_rotation=True, class_balance=True):\n",
    "        \"\"\"\n",
    "        ModelNet40 Dataset for 3D point cloud classification\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        root_dir: str, path to the ModelNet40 dataset\n",
    "        split: str, 'train' or 'test'\n",
    "        num_points: int, number of points to sample (1024, 2048, or 4096)\n",
    "        transform: callable, optional transform to be applied on a sample\n",
    "        random_rotation: bool, whether to apply random rotation augmentation\n",
    "        class_balance: bool, whether to use class balancing\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.num_points = num_points\n",
    "        self.transform = transform\n",
    "        self.random_rotation = random_rotation\n",
    "        \n",
    "        # Get all class folders\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Get all model files\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls_name, split)\n",
    "            if os.path.exists(cls_path):\n",
    "                model_files = glob.glob(os.path.join(cls_path, '*.off'))\n",
    "                self.files.extend(model_files)\n",
    "                self.labels.extend([self.class_to_idx[cls_name]] * len(model_files))\n",
    "        \n",
    "        # Prepare weights for class balancing\n",
    "        self.weights = None\n",
    "        if class_balance:\n",
    "            # Count instances per class\n",
    "            class_counts = np.zeros(len(self.classes))\n",
    "            for label in self.labels:\n",
    "                class_counts[label] += 1\n",
    "            # Calculate weights inversely proportional to class frequency\n",
    "            self.weights = 1.0 / class_counts\n",
    "            # Assign weight to each sample\n",
    "            self.sample_weights = np.array([self.weights[label] for label in self.labels])\n",
    "            self.sample_weights = torch.from_numpy(self.sample_weights).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load point cloud (placeholder - actual loading depends on file format)\n",
    "        # For OFF files, you'd parse the OFF format\n",
    "        # For this example, we'll generate a random point cloud\n",
    "        # In practice, replace this with actual file loading\n",
    "        points = np.random.rand(1024, 3)  # Placeholder\n",
    "        \n",
    "        # Sample num_points points\n",
    "        if points.shape[0] > self.num_points:\n",
    "            indices = np.random.choice(points.shape[0], self.num_points, replace=False)\n",
    "            points = points[indices]\n",
    "        elif points.shape[0] < self.num_points:\n",
    "            # Upsample by duplicating\n",
    "            indices = np.random.choice(points.shape[0], self.num_points - points.shape[0], replace=True)\n",
    "            points = np.vstack([points, points[indices]])\n",
    "        \n",
    "        # Normalize to unit sphere\n",
    "        center = np.mean(points, axis=0)\n",
    "        points = points - center\n",
    "        dist = np.max(np.sqrt(np.sum(points ** 2, axis=1)))\n",
    "        points = points / dist\n",
    "        \n",
    "        # Apply random rotation\n",
    "        if self.split == 'train' and self.random_rotation:\n",
    "            # Random rotation around z-axis\n",
    "            theta = np.random.uniform(0, 2 * np.pi)\n",
    "            rotation_matrix = np.array([\n",
    "                [np.cos(theta), -np.sin(theta), 0],\n",
    "                [np.sin(theta), np.cos(theta), 0],\n",
    "                [0, 0, 1]\n",
    "            ])\n",
    "            points = points @ rotation_matrix\n",
    "        \n",
    "        if self.transform:\n",
    "            points = self.transform(points)\n",
    "        \n",
    "        return points, label\n",
    "\n",
    "class PointCloudCorruptor:\n",
    "    \"\"\"Class for adding various environmental corruptions to point clouds\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_snow(points, density=0.1, scatter_strength=0.05):\n",
    "        \"\"\"Add snow effect to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        density: float, snow density (0.0-1.0)\n",
    "        scatter_strength: float, scattering effect strength\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        num_points = points.shape[0]\n",
    "        num_snow_points = int(num_points * density)\n",
    "        \n",
    "        # Generate snow points\n",
    "        snow_points = np.random.uniform(-1, 1, size=(num_snow_points, 3))\n",
    "        \n",
    "        # Create scattering effect\n",
    "        scatter = np.random.normal(0, scatter_strength, size=(num_points, 3))\n",
    "        points_scattered = points + scatter\n",
    "        \n",
    "        # Randomly select points to replace with snow\n",
    "        indices = np.random.choice(num_points, num_snow_points, replace=False)\n",
    "        points_scattered[indices] = snow_points\n",
    "        \n",
    "        return points_scattered\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_rain(points, density=0.1, drop_length=0.05):\n",
    "        \"\"\"Add rain effect to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        density: float, rain density (0.0-1.0)\n",
    "        drop_length: float, length of rain drops\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        num_points = points.shape[0]\n",
    "        num_rain_points = int(num_points * density)\n",
    "        \n",
    "        # Generate rain points (starting positions)\n",
    "        rain_points = np.random.uniform(-1, 1, size=(num_rain_points, 3))\n",
    "        # Make rain streaks by extending in mostly downward direction\n",
    "        rain_directions = np.random.normal(0, 0.1, size=(num_rain_points, 2))\n",
    "        rain_directions = np.column_stack([rain_directions, -np.abs(np.random.normal(0, 0.5, num_rain_points))])\n",
    "        rain_directions /= np.linalg.norm(rain_directions, axis=1, keepdims=True)\n",
    "        \n",
    "        # Create rain droplet streaks\n",
    "        rain_streaks = rain_points + rain_directions * drop_length\n",
    "        \n",
    "        # Randomly select points to replace with rain\n",
    "        indices = np.random.choice(num_points, num_rain_points, replace=False)\n",
    "        points_with_rain = points.copy()\n",
    "        points_with_rain[indices] = rain_streaks\n",
    "        \n",
    "        return points_with_rain\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_fog(points, density=0.2):\n",
    "        \"\"\"Add fog effect to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        density: float, fog density (0.0-1.0)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        num_points = points.shape[0]\n",
    "        \n",
    "        # Calculate distance from origin for each point\n",
    "        distances = np.linalg.norm(points, axis=1)\n",
    "        \n",
    "        # Attenuate points based on distance and fog density\n",
    "        attenuation = np.exp(-density * distances)\n",
    "        \n",
    "        # Apply attenuation: closer to 0 means more fog effect\n",
    "        # We'll add random displacement proportional to fog intensity\n",
    "        fog_displacement = (1 - attenuation).reshape(-1, 1) * np.random.normal(0, 0.1, size=(num_points, 3))\n",
    "        points_with_fog = points + fog_displacement\n",
    "        \n",
    "        # Randomly drop some distant points (completely obscured by fog)\n",
    "        dropout_prob = 1 - np.exp(-density * distances * 2)\n",
    "        dropout_mask = np.random.random(num_points) > dropout_prob\n",
    "        \n",
    "        # Create a mix of original and fogged points\n",
    "        return points_with_fog * dropout_mask.reshape(-1, 1) + (1 - dropout_mask.reshape(-1, 1)) * np.random.uniform(-0.1, 0.1, size=(num_points, 3))\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(points, sigma=0.02):\n",
    "        \"\"\"Add Gaussian noise to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        sigma: float, standard deviation relative to object size\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(0, sigma, size=points.shape)\n",
    "        return points + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_depth_noise(points, k=0.05):\n",
    "        \"\"\"Add depth-dependent noise (more noise at greater distances)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        k: float, noise coefficient\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        # Calculate distance from origin\n",
    "        distances = np.linalg.norm(points, axis=1)\n",
    "        \n",
    "        # Scale noise by distance (farther = more noise)\n",
    "        noise_scale = k * distances.reshape(-1, 1)\n",
    "        noise = np.random.normal(0, 1, size=points.shape) * noise_scale\n",
    "        \n",
    "        return points + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_occlusion(points, ratio=0.2, mode='random'):\n",
    "        \"\"\"Add occlusion to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        ratio: float, percentage of points to occlude (0.0-1.0)\n",
    "        mode: str, 'random', 'region', or 'semantic'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        num_points = points.shape[0]\n",
    "        num_to_occlude = int(num_points * ratio)\n",
    "        \n",
    "        if mode == 'random':\n",
    "            # Random occlusion\n",
    "            mask = np.ones(num_points, dtype=bool)\n",
    "            indices = np.random.choice(num_points, num_to_occlude, replace=False)\n",
    "            mask[indices] = False\n",
    "            return points[mask]\n",
    "        \n",
    "        elif mode == 'region':\n",
    "            # Region-based occlusion\n",
    "            center = np.random.uniform(-0.5, 0.5, size=3)\n",
    "            radius = np.random.uniform(0.2, 0.5)\n",
    "            \n",
    "            # Calculate distances to the center\n",
    "            distances = np.linalg.norm(points - center, axis=1)\n",
    "            \n",
    "            # Keep points outside the sphere\n",
    "            mask = distances > radius\n",
    "            return points[mask]\n",
    "        \n",
    "        elif mode == 'semantic':\n",
    "            # Semantic occlusion (simplified - just occludes one side)\n",
    "            # In real implementation, this would target specific semantic parts\n",
    "            dimension = np.random.randint(0, 3)  # Choose x, y, or z\n",
    "            threshold = np.median(points[:, dimension])\n",
    "            mask = points[:, dimension] > threshold\n",
    "            return points[mask]\n",
    "        \n",
    "        return points\n",
    "\n",
    "def prepare_dataloaders(dataset_path, batch_size=32, num_points=1024, num_workers=4):\n",
    "    \"\"\"\n",
    "    Prepare train and test data loaders\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_path: str, path to ModelNet40 dataset\n",
    "    batch_size: int, batch size\n",
    "    num_points: int, number of points per sample\n",
    "    num_workers: int, number of dataloader workers\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_loader: DataLoader for training data\n",
    "    test_loader: DataLoader for test data\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = ModelNet40Dataset(\n",
    "        root_dir=dataset_path,\n",
    "        split='train',\n",
    "        num_points=num_points,\n",
    "        random_rotation=True,\n",
    "        class_balance=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = ModelNet40Dataset(\n",
    "        root_dir=dataset_path,\n",
    "        split='test',\n",
    "        num_points=num_points,\n",
    "        random_rotation=False,\n",
    "        class_balance=False\n",
    "    )\n",
    "    \n",
    "    # Create weighted sampler for class balancing\n",
    "    if train_dataset.weights is not None:\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=train_dataset.sample_weights,\n",
    "            num_samples=len(train_dataset),\n",
    "            replacement=True\n",
    "        )\n",
    "    else:\n",
    "        sampler = None\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_corrupted_dataset(dataset, corruption_type, severity):\n",
    "    \"\"\"\n",
    "    Create a corrupted version of a dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset: Dataset object\n",
    "    corruption_type: str, type of corruption\n",
    "    severity: int or float, severity level\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    corrupted_dataset: Dataset with corruption applied\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataset\n",
    "    corrupted_dataset = copy.deepcopy(dataset)\n",
    "    \n",
    "    # Define corruption parameters based on severity (1-5 scale)\n",
    "    if corruption_type == 'gaussian_noise':\n",
    "        param = {1: 0.01, 2: 0.02, 3: 0.03, 4: 0.04, 5: 0.05}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_gaussian_noise(x, sigma=param)\n",
    "    \n",
    "    elif corruption_type == 'snow':\n",
    "        density = {1: 0.05, 2: 0.1, 3: 0.15, 4: 0.2, 5: 0.3}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_snow(x, density=density)\n",
    "    \n",
    "    elif corruption_type == 'rain':\n",
    "        density = {1: 0.05, 2: 0.1, 3: 0.15, 4: 0.2, 5: 0.3}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_rain(x, density=density)\n",
    "    \n",
    "    elif corruption_type == 'fog':\n",
    "        density = {1: 0.05, 2: 0.1, 3: 0.2, 4: 0.3, 5: 0.4}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_fog(x, density=density)\n",
    "    \n",
    "    elif corruption_type == 'depth_noise':\n",
    "        param = {1: 0.01, 2: 0.02, 3: 0.04, 4: 0.06, 5: 0.1}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_depth_noise(x, k=param)\n",
    "    \n",
    "    elif corruption_type == 'occlusion':\n",
    "        ratio = {1: 0.1, 2: 0.15, 3: 0.2, 4: 0.25, 5: 0.3}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_occlusion(x, ratio=ratio)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported corruption type: {corruption_type}\")\n",
    "    \n",
    "    # Set the transform in the corrupted dataset\n",
    "    corrupted_dataset.transform = transform\n",
    "    \n",
    "    return corrupted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PointCloudAttacker:\n",
    "    def __init__(self, model, device=None):\n",
    "        \"\"\"\n",
    "        Class for implementing various adversarial attacks on point cloud models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model: torch.nn.Module, the target model to attack\n",
    "        device: torch.device, device to perform computations on\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    # ======== White-box Attacks ========\n",
    "    \n",
    "    def fgsm_attack(self, points, labels, epsilon=0.03):\n",
    "        \"\"\"\n",
    "        Fast Gradient Sign Method attack\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: torch.Tensor of shape (B, N, 3) or (B, 3, N)\n",
    "        labels: torch.Tensor of shape (B,)\n",
    "        epsilon: float, attack strength\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        perturbed_points: torch.Tensor, adversarial examples\n",
    "        \"\"\"\n",
    "        # Clone the input and make sure it requires grad\n",
    "        points_format = 'channels_last' if points.shape[1] == 3 and len(points.shape) == 3 else 'channels_first'\n",
    "        if points_format == 'channels_first':\n",
    "            x = points.clone().detach().requires_grad_(True)\n",
    "        else:\n",
    "            x = points.clone().detach().transpose(2, 1).requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(x)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Generate perturbation\n",
    "        data_grad = x.grad.data\n",
    "        sign_data_grad = data_grad.sign()\n",
    "        \n",
    "        # Create perturbed data\n",
    "        perturbed_points = x + epsilon * sign_data_grad\n",
    "        \n",
    "        # Project perturbations to keep points within original bounds\n",
    "        # Assuming original points are normalized to unit sphere\n",
    "        if points_format == 'channels_last':\n",
    "            return perturbed_points.detach().transpose(2, 1)\n",
    "        return perturbed_points.detach()\n",
    "    \n",
    "    def pgd_attack(self, points, labels, epsilon=0.03, alpha=None, num_iter=20):\n",
    "        \"\"\"\n",
    "        Projected Gradient Descent attack\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: torch.Tensor of shape (B, N, 3) or (B, 3, N)\n",
    "        labels: torch.Tensor of shape (B,)\n",
    "        epsilon: float, attack strength\n",
    "        alpha: float, step size (if None, will be set to epsilon/4)\n",
    "        num_iter: int, number of iterations\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        perturbed_points: torch.Tensor, adversarial examples\n",
    "        \"\"\"\n",
    "        if alpha is None:\n",
    "            alpha = epsilon / 4\n",
    "            \n",
    "        points_format = 'channels_last' if points.shape[1] == 3 and len(points.shape) == 3 else 'channels_first'\n",
    "        if points_format == 'channels_last':\n",
    "            original_points = points.clone().detach().transpose(2, 1)\n",
    "            perturbed_points = original_points.clone().detach().requires_grad_(True)\n",
    "        else:\n",
    "            original_points = points.clone().detach()\n",
    "            perturbed_points = original_points.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        for i in range(num_iter):\n",
    "            # Forward pass\n",
    "            outputs = self.model(perturbed_points)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update points\n",
    "            with torch.no_grad():\n",
    "                perturbation = alpha * perturbed_points.grad.sign()\n",
    "                perturbed_points = perturbed_points + perturbation\n",
    "                \n",
    "                # Project back to epsilon ball\n",
    "                delta = perturbed_points - original_points\n",
    "                delta = torch.clamp(delta, -epsilon, epsilon)\n",
    "                perturbed_points = original_points + delta\n",
    "                \n",
    "                # Project to unit sphere if needed (optional)\n",
    "                # norm = torch.norm(perturbed_points, dim=1, keepdim=True)\n",
    "                # perturbed_points = perturbed_points / torch.max(norm, torch.ones_like(norm))\n",
    "            \n",
    "            # Reset gradients\n",
    "            if i < num_iter - 1:\n",
    "                perturbed_points.requires_grad_(True)\n",
    "        \n",
    "        if points_format == 'channels_last':\n",
    "            return perturbed_points.detach().transpose(2, 1)\n",
    "        return perturbed_points.detach()\n",
    "    \n",
    "    def cw_attack(self, points, labels, confidence=0, lr=0.01, num_iter=100):\n",
    "        \"\"\"\n",
    "        Carlini & Wagner attack\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: torch.Tensor of shape (B, N, 3) or (B, 3, N)\n",
    "        labels: torch.Tensor of shape (B,)\n",
    "        confidence: float, confidence parameter κ\n",
    "        lr: float, learning rate\n",
    "        num_iter: int, number of iterations\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        perturbed_points: torch.Tensor, adversarial examples\n",
    "        \"\"\"\n",
    "        points_format = 'channels_last' if points.shape[1] == 3 and len(points.shape) == 3 else 'channels_first'\n",
    "        if points_format == 'channels_last':\n",
    "            original_points = points.clone().detach().transpose(2, 1)\n",
    "        else:\n",
    "            original_points = points.clone().detach()\n",
    "            \n",
    "        # Initialize perturbation\n",
    "        delta = torch.zeros_like(original_points, requires_grad=True, device=self.device)\n",
    "        optimizer = torch.optim.Adam([delta], lr=lr)\n",
    "        \n",
    "        batch_size = points.shape[0]\n",
    "        target_labels = labels  # For untargeted attack, we just use the original labels\n",
    "        \n",
    "        for i in range(num_iter):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Apply perturbation\n",
    "            perturbed_points = original_points + delta\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(perturbed_points)\n",
    "            \n",
    "            # CW loss: maximize the difference between target and highest non-target class\n",
    "            target_values = outputs.gather(1, target_labels.unsqueeze(1)).squeeze(1)\n",
    "            other_values = outputs.clone()\n",
    "            other_values.scatter_(1, target_labels.unsqueeze(1), -float('inf'))\n",
    "            other_values = other_values.max(dim=1)[0]\n",
    "            \n",
    "            # Loss with confidence parameter κ\n",
    "            loss = (other_values - target_values + confidence).clamp(min=0).mean()\n",
    "            \n",
    "            # Add regularization term for perturbation magnitude\n",
    "            loss += 0.01 * torch.norm(delta, dim=[1, 2]).mean()\n",
    "            \n",
    "            # Backward and update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        perturbed_points = (original_points + delta).detach()\n",
    "        if points_format == 'channels_last':\n",
    "            return perturbed_points.transpose(2, 1)\n",
    "        return perturbed_points\n",
    "    \n",
    "    # ======== Black-box Attacks ========\n",
    "    \n",
    "    def transfer_attack(self, points, labels, surrogate_model, attack_type='pgd', **attack_params):\n",
    "        \"\"\"\n",
    "        Transfer attack using a surrogate model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: torch.Tensor of shape (B, N, 3) or (B, 3, N)\n",
    "        labels: torch.Tensor of shape (B,)\n",
    "        surrogate_model: torch.nn.Module, surrogate model to generate adversarial examples\n",
    "        attack_type: str, type of attack to use ('fgsm', 'pgd', 'cw')\n",
    "        attack_params: dict, parameters for the attack\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        perturbed_points: torch.Tensor, adversarial examples\n",
    "        \"\"\"\n",
    "        # Save original model\n",
    "        original_model = self.model\n",
    "        \n",
    "        # Set surrogate model for attack generation\n",
    "        self.model = surrogate_model\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        if attack_type == 'fgsm':\n",
    "            perturbed_points = self.fgsm_attack(points, labels, **attack_params)\n",
    "        elif attack_type == 'pgd':\n",
    "            perturbed_points = self.pgd_attack(points, labels, **attack_params)\n",
    "        elif attack_type == 'cw':\n",
    "            perturbed_points = self.cw_attack(points, labels, **attack_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported attack type: {attack_type}\")\n",
    "        \n",
    "        # Restore original model\n",
    "        self.model = original_model\n",
    "        \n",
    "        return perturbed_points\n",
    "    \n",
    "    # ======== Point Cloud-Specific Attacks ========\n",
    "    \n",
    "    def point_perturbation_attack(self, points, labels, max_displacement=0.05, num_iter=50, lr=0.01):\n",
    "        \"\"\"\n",
    "        Point perturbation attack with maximum displacement constraints\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: torch.Tensor of shape (B, N, 3) or (B, 3, N)\n",
    "        labels: torch.Tensor of shape (B,)\n",
    "        max_displacement: float, maximum displacement per point\n",
    "        num_iter: int, number of iterations\n",
    "        lr: float, learning rate\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        perturbed_points: torch.Tensor, adversarial examples\n",
    "        \"\"\"\n",
    "        points_format = 'channels_last' if points.shape[1] == 3 and len(points.shape) == 3 else 'channels_first'\n",
    "        if points_format == 'channels_last':\n",
    "            original_points = points.clone().detach().transpose(2, 1)\n",
    "        else:\n",
    "            original_points = points.clone().detach()\n",
    "            \n",
    "        # Initialize perturbation\n",
    "        delta = torch.zeros_like(original_points, requires_grad=True, device=self.device)\n",
    "        optimizer = torch.optim.Adam([delta], lr=lr)\n",
    "        \n",
    "        for i in range(num_iter):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Apply perturbation with constraints\n",
    "            delta_clamped = torch.clamp(delta, -max_displacement, max_displacement)\n",
    "            perturbed_points = original_points + delta_clamped\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(perturbed_points)\n",
    "            \n",
    "            # Use cross-entropy loss for untargeted attack\n",
    "            loss = -torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            \n",
    "            # Backward and update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Final clamping\n",
    "        delta_clamped = torch.clamp(delta, -max_displacement, max_displacement)\n",
    "        perturbed_points = (original_points + delta_clamped).detach()\n",
    "        \n",
    "        if points_format == 'channels_last':\n",
    "            return perturbed_points.transpose(2, 1)\n",
    "        return perturbed_points\n",
    "    \n",
    "    def point_addition_removal(self, points, labels, ratio=0.1, mode='removal'):\n",
    "        \"\"\"\n",
    "        Point addition or removal attack\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: torch.Tensor of shape (B, N, 3) or (B, 3, N)\n",
    "        labels: torch.Tensor of shape (B,)\n",
    "        ratio: float, percentage of points to add/remove\n",
    "        mode: str, 'addition' or 'removal'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        perturbed_points: torch.Tensor, adversarial examples\n",
    "        \"\"\"\n",
    "        points_format = 'channels_last' if points.shape[1] == 3 and len(points.shape) == 3 else 'channels_first'\n",
    "        if points_format == 'channels_last':\n",
    "            # Points are in shape [B, N, 3]\n",
    "            B, N, C = points.shape\n",
    "            points_to_process = points.clone()\n",
    "        else:\n",
    "            # Points are in shape [B, 3, N]\n",
    "            B, C, N = points.shape\n",
    "            points_to_process = points.clone().transpose(2, 1)\n",
    "            \n",
    "        perturbed_points = []\n",
    "        \n",
    "        for i in range(B):\n",
    "            point_cloud = points_to_process[i]  # [N, 3]\n",
    "            \n",
    "            if mode == 'removal':\n",
    "                # Remove points\n",
    "                num_to_remove = int(N * ratio)\n",
    "                \n",
    "                # Try to identify critical points (can be more sophisticated)\n",
    "                # For now, just randomly remove points\n",
    "                indices = torch.randperm(N)[:N-num_to_remove]\n",
    "                perturbed_cloud = point_cloud[indices]\n",
    "                \n",
    "                # Pad back to original size by duplicating existing points\n",
    "                if len(perturbed_cloud) < N:\n",
    "                    pad_indices = torch.randint(0, len(perturbed_cloud), (N - len(perturbed_cloud),))\n",
    "                    padding = perturbed_cloud[pad_indices]\n",
    "                    perturbed_cloud = torch.cat([perturbed_cloud, padding], dim=0)\n",
    "                \n",
    "            elif mode == 'addition':\n",
    "                # Add points\n",
    "                num_to_add = int(N * ratio)\n",
    "                \n",
    "                # Create new points (could be more sophisticated)\n",
    "                # For now, add noise to existing points\n",
    "                indices = torch.randint(0, N, (num_to_add,))\n",
    "                new_points = point_cloud[indices] + torch.randn_like(point_cloud[indices]) * 0.1\n",
    "                \n",
    "                # Remove some original points to keep the total constant\n",
    "                keep_indices = torch.randperm(N)[:N-num_to_add]\n",
    "                perturbed_cloud = torch.cat([point_cloud[keep_indices], new_points], dim=0)\n",
    "                \n",
    "            perturbed_points.append(perturbed_cloud)\n",
    "            \n",
    "        perturbed_points = torch.stack(perturbed_points, dim=0)\n",
    "        \n",
    "        if points_format == 'channels_first':\n",
    "            return perturbed_points.transpose(2, 1)\n",
    "        return perturbed_points\n",
    "    \n",
    "    # ======== Evaluation Metrics ========\n",
    "    \n",
    "    @staticmethod\n",
    "    def chamfer_distance(x, y):\n",
    "        \"\"\"\n",
    "        Calculate Chamfer distance between two point clouds\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: torch.Tensor of shape (B, N, 3)\n",
    "        y: torch.Tensor of shape (B, M, 3)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        distance: torch.Tensor of shape (B,)\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(2)  # (B, N, 1, 3)\n",
    "        y = y.unsqueeze(1)  # (B, 1, M, 3)\n",
    "        \n",
    "        # Calculate pairwise distances\n",
    "        dist = torch.sum((x - y) ** 2, dim=3)  # (B, N, M)\n",
    "        \n",
    "        # Find minimum distances\n",
    "        min_dist_xy = torch.min(dist, dim=2)[0]  # (B, N)\n",
    "        min_dist_yx = torch.min(dist, dim=1)[0]  # (B, M)\n",
    "        \n",
    "        # Calculate Chamfer distance\n",
    "        chamfer_dist = torch.mean(min_dist_xy, dim=1) + torch.mean(min_dist_yx, dim=1)\n",
    "        \n",
    "        return chamfer_dist\n",
    "    \n",
    "    @staticmethod\n",
    "    def hausdorff_distance(x, y):\n",
    "        \"\"\"\n",
    "        Calculate Hausdorff distance between two point clouds\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: torch.Tensor of shape (B, N, 3)\n",
    "        y: torch.Tensor of shape (B, M, 3)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        distance: torch.Tensor of shape (B,)\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(2)  # (B, N, 1, 3)\n",
    "        y = y.unsqueeze(1)  # (B, 1, M, 3)\n",
    "        \n",
    "        # Calculate pairwise distances\n",
    "        dist = torch.sum((x - y) ** 2, dim=3)  # (B, N, M)\n",
    "        \n",
    "        # Find minimum distances\n",
    "        min_dist_xy = torch.min(dist, dim=2)[0]  # (B, N)\n",
    "        min_dist_yx = torch.min(dist, dim=1)[0]  # (B, M)\n",
    "        \n",
    "        # Calculate Hausdorff distance\n",
    "        hausdorff_dist = torch.max(torch.max(min_dist_xy, dim=1)[0], torch.max(min_dist_yx, dim=1)[0])\n",
    "        \n",
    "        return hausdorff_dist\n",
    "\n",
    "class AdversarialEvaluator:\n",
    "    def __init__(self, model_list, device=None):\n",
    "        \"\"\"\n",
    "        Class for evaluating model robustness against adversarial attacks\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_list: dict, dictionary of models to evaluate {name: model}\n",
    "        device: torch.device, device to perform computations on\n",
    "        \"\"\"\n",
    "        self.models = model_list\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.attackers = {name: PointCloudAttacker(model, self.device) for name, model in self.models.items()}\n",
    "        \n",
    "    def evaluate_whitebox_attacks(self, dataloader, attack_params=None):\n",
    "        \"\"\"\n",
    "        Evaluate models against white-box attacks\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataloader: torch.utils.data.DataLoader, test data\n",
    "        attack_params: dict, parameters for different attacks\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        results: dict, evaluation results\n",
    "        \"\"\"\n",
    "        if attack_params is None:\n",
    "            attack_params = {\n",
    "                'fgsm': {'epsilon': [0.01, 0.03, 0.05, 0.1]},\n",
    "                'pgd': {\n",
    "                    'epsilon': [0.03, 0.05],\n",
    "                    'num_iter': [10, 20, 50],\n",
    "                    'alpha': None  # Will be set to epsilon/4\n",
    "                },\n",
    "                'cw': {'confidence': [0, 20, 40], 'num_iter': 100}\n",
    "            }\n",
    "            \n",
    "        results = {}\n",
    "        \n",
    "        # Evaluate each model\n",
    "        for model_name, attacker in self.attackers.items():\n",
    "            model_results = {}\n",
    "            model = self.models[model_name]\n",
    "            model.eval()\n",
    "            \n",
    "            print(f\"Evaluating {model_name} against white-box attacks...\")\n",
    "            \n",
    "            # FGSM attacks\n",
    "            for eps in attack_params['fgsm']['epsilon']:\n",
    "                clean_correct = 0\n",
    "                adv_correct = 0\n",
    "                total = 0\n",
    "                chamfer_dists = []\n",
    "                hausdorff_dists = []\n",
    "                \n",
    "                for data, labels in tqdm(dataloader, desc=f\"FGSM ε={eps}\"):\n",
    "                    data, labels = data.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    # Calculate clean accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        clean_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    # Generate adversarial examples\n",
    "                    perturbed_data = attacker.fgsm_attack(data, labels, epsilon=eps)\n",
    "                    \n",
    "                    # Calculate adversarial accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(perturbed_data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        adv_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    # Calculate distances\n",
    "                    if data.shape[1] != 3:  # If data is in shape [B, 3, N]\n",
    "                        data_reshaped = data.transpose(2, 1)\n",
    "                        perturbed_reshaped = perturbed_data.transpose(2, 1)\n",
    "                    else:\n",
    "                        data_reshaped = data\n",
    "                        perturbed_reshaped = perturbed_data\n",
    "                        \n",
    "                    chamfer_dists.append(attacker.chamfer_distance(data_reshaped, perturbed_reshaped).mean().item())\n",
    "                    hausdorff_dists.append(attacker.hausdorff_distance(data_reshaped, perturbed_reshaped).mean().item())\n",
    "                    \n",
    "                    total += labels.size(0)\n",
    "                \n",
    "                model_results[f'fgsm_eps_{eps}'] = {\n",
    "                    'clean_acc': clean_correct / total,\n",
    "                    'adv_acc': adv_correct / total,\n",
    "                    'chamfer_dist': np.mean(chamfer_dists),\n",
    "                    'hausdorff_dist': np.mean(hausdorff_dists)\n",
    "                }\n",
    "                \n",
    "            # PGD attacks\n",
    "            for eps in attack_params['pgd']['epsilon']:\n",
    "                for iters in attack_params['pgd']['num_iter']:\n",
    "                    alpha = eps / 4  # Default step size\n",
    "                    \n",
    "                    clean_correct = 0\n",
    "                    adv_correct = 0\n",
    "                    total = 0\n",
    "                    chamfer_dists = []\n",
    "                    hausdorff_dists = []\n",
    "                    \n",
    "                    for data, labels in tqdm(dataloader, desc=f\"PGD ε={eps}, iter={iters}\"):\n",
    "                        data, labels = data.to(self.device), labels.to(self.device)\n",
    "                        \n",
    "                        # Calculate clean accuracy\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(data)\n",
    "                            _, predicted = outputs.max(1)\n",
    "                            clean_correct += predicted.eq(labels).sum().item()\n",
    "                        \n",
    "                        # Generate adversarial examples\n",
    "                        perturbed_data = attacker.pgd_attack(data, labels, epsilon=eps, alpha=alpha, num_iter=iters)\n",
    "                        \n",
    "                        # Calculate adversarial accuracy\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(perturbed_data)\n",
    "                            _, predicted = outputs.max(1)\n",
    "                            adv_correct += predicted.eq(labels).sum().item()\n",
    "                        \n",
    "                        # Calculate distances\n",
    "                        if data.shape[1] != 3:  # If data is in shape [B, 3, N]\n",
    "                            data_reshaped = data.transpose(2, 1)\n",
    "                            perturbed_reshaped = perturbed_data.transpose(2, 1)\n",
    "                        else:\n",
    "                            data_reshaped = data\n",
    "                            perturbed_reshaped = perturbed_data\n",
    "                            \n",
    "                        chamfer_dists.append(attacker.chamfer_distance(data_reshaped, perturbed_reshaped).mean().item())\n",
    "                        hausdorff_dists.append(attacker.hausdorff_distance(data_reshaped, perturbed_reshaped).mean().item())\n",
    "                        \n",
    "                        total += labels.size(0)\n",
    "                    \n",
    "                    model_results[f'pgd_eps_{eps}_iter_{iters}'] = {\n",
    "                        'clean_acc': clean_correct / total,\n",
    "                        'adv_acc': adv_correct / total,\n",
    "                        'chamfer_dist': np.mean(chamfer_dists),\n",
    "                        'hausdorff_dist': np.mean(hausdorff_dists)\n",
    "                    }\n",
    "            \n",
    "            # C&W attacks - more computation intensive, let's use a subset of the data\n",
    "            for conf in attack_params['cw']['confidence']:\n",
    "                clean_correct = 0\n",
    "                adv_correct = 0\n",
    "                total = 0\n",
    "                chamfer_dists = []\n",
    "                hausdorff_dists = []\n",
    "                \n",
    "                # Limit to first 200 samples for C&W due to computational cost\n",
    "                sample_count = 0\n",
    "                for data, labels in tqdm(dataloader, desc=f\"C&W κ={conf}\"):\n",
    "                    if sample_count >= 200:\n",
    "                        break\n",
    "                        \n",
    "                    data, labels = data.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    # Calculate clean accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        clean_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    # Generate adversarial examples\n",
    "                    perturbed_data = attacker.cw_attack(data, labels, confidence=conf, num_iter=attack_params['cw']['num_iter'])\n",
    "                    \n",
    "                    # Calculate adversarial accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(perturbed_data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        adv_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    # Calculate distances\n",
    "                    if data.shape[1] != 3:  # If data is in shape [B, 3, N]\n",
    "                        data_reshaped = data.transpose(2, 1)\n",
    "                        perturbed_reshaped = perturbed_data.transpose(2, 1)\n",
    "                    else:\n",
    "                        data_reshaped = data\n",
    "                        perturbed_reshaped = perturbed_data\n",
    "                        \n",
    "                    chamfer_dists.append(attacker.chamfer_distance(data_reshaped, perturbed_reshaped).mean().item())\n",
    "                    hausdorff_dists.append(attacker.hausdorff_distance(data_reshaped, perturbed_reshaped).mean().item())\n",
    "                    \n",
    "                    total += labels.size(0)\n",
    "                    sample_count += labels.size(0)\n",
    "                \n",
    "                if total > 0:\n",
    "                    model_results[f'cw_conf_{conf}'] = {\n",
    "                        'clean_acc': clean_correct / total,\n",
    "                        'adv_acc': adv_correct / total,\n",
    "                        'chamfer_dist': np.mean(chamfer_dists),\n",
    "                        'hausdorff_dist': np.mean(hausdorff_dists)\n",
    "                    }\n",
    "            \n",
    "            results[model_name] = model_results\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def evaluate_point_specific_attacks(self, dataloader):\n",
    "        \"\"\"\n",
    "        Evaluate models against point cloud-specific attacks\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataloader: torch.utils.data.DataLoader, test data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        results: dict, evaluation results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Define attack parameters\n",
    "        perturbation_max = [0.02, 0.05, 0.1]\n",
    "        removal_ratios = [0.05, 0.1, 0.15]\n",
    "        \n",
    "        # Evaluate each model\n",
    "        for model_name, attacker in self.attackers.items():\n",
    "            model_results = {}\n",
    "            model = self.models[model_name]\n",
    "            model.eval()\n",
    "            \n",
    "            print(f\"Evaluating {model_name} against point cloud-specific attacks...\")\n",
    "            \n",
    "            # Point perturbation attacks\n",
    "            for max_disp in perturbation_max:\n",
    "                clean_correct = 0\n",
    "                adv_correct = 0\n",
    "                total = 0\n",
    "                chamfer_dists = []\n",
    "                \n",
    "                for data, labels in tqdm(dataloader, desc=f\"Perturbation max={max_disp}\"):\n",
    "                    data, labels = data.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    # Calculate clean accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        clean_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    # Generate adversarial examples\n",
    "                    perturbed_data = attacker.point_perturbation_attack(data, labels, max_displacement=max_disp)\n",
    "                    \n",
    "                    # Calculate adversarial accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(perturbed_data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        adv_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    # Calculate Chamfer distance\n",
    "                    if data.shape[1] != 3:  # If data is in shape [B, 3, N]\n",
    "                        data_reshaped = data.transpose(2, 1)\n",
    "                        perturbed_reshaped = perturbed_data.transpose(2, 1)\n",
    "                    else:\n",
    "                        data_reshaped = data\n",
    "                        perturbed_reshaped = perturbed_data\n",
    "                        \n",
    "                    chamfer_dists.append(attacker.chamfer_distance(data_reshaped, perturbed_reshaped).mean().item())\n",
    "                    \n",
    "                    total += labels.size(0)\n",
    "                \n",
    "                model_results[f'perturb_max_{max_disp}'] = {\n",
    "                    'clean_acc': clean_correct / total,\n",
    "                    'adv_acc': adv_correct / total,\n",
    "                    'chamfer_dist': np.mean(chamfer_dists)\n",
    "                }\n",
    "            \n",
    "            # Point removal attacks\n",
    "            for ratio in removal_ratios:\n",
    "                clean_correct = 0\n",
    "                adv_correct = 0\n",
    "                total = 0\n",
    "                \n",
    "                for data, labels in tqdm(dataloader, desc=f\"Removal ratio={ratio}\"):\n",
    "                    data, labels = data.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    # Calculate clean accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        clean_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    # Generate adversarial examples\n",
    "                    perturbed_data = attacker.point_addition_removal(data, labels, ratio=ratio, mode='removal')\n",
    "                    \n",
    "                    # Calculate adversarial accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(perturbed_data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        adv_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    total += labels.size(0)\n",
    "                \n",
    "                model_results[f'removal_ratio_{ratio}'] = {\n",
    "                    'clean_acc': clean_correct / total,\n",
    "                    'adv_acc': adv_correct / total\n",
    "                }\n",
    "            \n",
    "            results[model_name] = model_results\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def cross_validate_attacks(self, dataset, attack_type, folds=5, **attack_params):\n",
    "        \"\"\"\n",
    "        Perform cross-validation for adversarial attacks\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset: torch.utils.data.Dataset, full dataset\n",
    "        attack_type: str, attack type to evaluate\n",
    "        folds: int, number of folds for cross-validation\n",
    "        attack_params: dict, parameters for the attack\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        cv_results: dict, cross-validation results\n",
    "        \"\"\"\n",
    "        # Define fold sizes\n",
    "        dataset_size = len(dataset)\n",
    "        fold_size = dataset_size // folds\n",
    "        \n",
    "        cv_results = {model_name: [] for model_name in self.models.keys()}\n",
    "        \n",
    "        for fold in range(folds):\n",
    "            print(f\"Cross-validation fold {fold+1}/{folds}\")\n",
    "            \n",
    "            # Split dataset\n",
    "            test_indices = list(range(fold * fold_size, (fold + 1) * fold_size))\n",
    "            train_indices = list(set(range(dataset_size)) - set(test_indices))\n",
    "            \n",
    "            test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "            test_loader = torch.utils.data.DataLoader(dataset, batch_size=32, sampler=test_sampler)\n",
    "            \n",
    "            # Evaluate each model\n",
    "            for model_name, model in self.models.items():\n",
    "                model.eval()\n",
    "                attacker = self.attackers[model_name]\n",
    "                \n",
    "                # Initialize attack function based on type\n",
    "                if attack_type == 'fgsm':\n",
    "                    attack_fn = attacker.fgsm_attack\n",
    "                elif attack_type == 'pgd':\n",
    "                    attack_fn = attacker.pgd_attack\n",
    "                elif attack_type == 'cw':\n",
    "                    attack_fn = attacker.cw_attack\n",
    "                elif attack_type == 'perturb':\n",
    "                    attack_fn = attacker.point_perturbation_attack\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported attack type for cross-validation: {attack_type}\")\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                clean_correct = 0\n",
    "                adv_correct = 0\n",
    "                total = 0\n",
    "                \n",
    "                for data, labels in test_loader:\n",
    "                    data, labels = data.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    # Calculate clean accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        clean_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    # Generate adversarial examples\n",
    "                    perturbed_data = attack_fn(data, labels, **attack_params)\n",
    "                    \n",
    "                    # Calculate adversarial accuracy\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(perturbed_data)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        adv_correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "                    total += labels.size(0)\n",
    "                \n",
    "                fold_result = {\n",
    "                    'fold': fold,\n",
    "                    'clean_acc': clean_correct / total,\n",
    "                    'adv_acc': adv_correct / total\n",
    "                }\n",
    "                cv_results[model_name].append(fold_result)\n",
    "                \n",
    "        # Calculate mean and std across folds\n",
    "        for model_name in cv_results:\n",
    "            clean_accs = [fold['clean_acc'] for fold in cv_results[model_name]]\n",
    "            adv_accs = [fold['adv_acc'] for fold in cv_results[model_name]]\n",
    "            \n",
    "            cv_results[model_name].append({\n",
    "                'mean_clean_acc': np.mean(clean_accs),\n",
    "                'std_clean_acc': np.std(clean_accs),\n",
    "                'mean_adv_acc': np.mean(adv_accs),\n",
    "                'std_adv_acc': np.std(adv_accs)\n",
    "            })\n",
    "            \n",
    "        return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
