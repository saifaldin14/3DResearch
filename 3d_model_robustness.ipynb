{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAbstractionLayer(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp):\n",
    "        super(SetAbstractionLayer, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        \n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        \n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, N, 3]\n",
    "            points: input points data, [B, N, D]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, S, 3]\n",
    "            new_points_concat: sample points feature data, [B, S, D']\n",
    "        \"\"\"\n",
    "        # Simplified implementation - in a real model this would use farthest point sampling\n",
    "        # and ball query operations from PointNet++ \n",
    "        B, N, C = xyz.shape\n",
    "        if self.npoint is None:\n",
    "            S = N\n",
    "        else:\n",
    "            S = min(self.npoint, N)\n",
    "            \n",
    "        # Simplified: just use the first S points instead of FPS\n",
    "        new_xyz = xyz[:, :S, :]\n",
    "        \n",
    "        # Simplified feature aggregation (in real implementation would be grouped by radius)\n",
    "        if points is not None:\n",
    "            new_points = points[:, :S, :]\n",
    "        else:\n",
    "            new_points = new_xyz\n",
    "            \n",
    "        # Apply MLP - simplified for this example\n",
    "        new_points = new_points.permute(0, 2, 1).unsqueeze(-1)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points = torch.relu(bn(conv(new_points)))\n",
    "        \n",
    "        new_points = new_points.squeeze(-1).permute(0, 2, 1)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "class PointNet2Classification(nn.Module):\n",
    "    def __init__(self, num_classes=40, normal_channel=False):\n",
    "        super(PointNet2Classification, self).__init__()\n",
    "        in_channel = 6 if normal_channel else 3\n",
    "        \n",
    "        self.normal_channel = normal_channel\n",
    "        \n",
    "        # SA modules\n",
    "        self.sa1 = SetAbstractionLayer(npoint=512, radius=0.2, nsample=32, \n",
    "                                        in_channel=in_channel, mlp=[64, 64, 128])\n",
    "        self.sa2 = SetAbstractionLayer(npoint=128, radius=0.4, nsample=64, \n",
    "                                        in_channel=128 + 3, mlp=[128, 128, 256])\n",
    "        self.sa3 = SetAbstractionLayer(npoint=None, radius=None, nsample=None, \n",
    "                                        in_channel=256 + 3, mlp=[256, 512, 1024])\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        B, _, _ = xyz.shape\n",
    "        if self.normal_channel:\n",
    "            norm = xyz[:, 3:, :]\n",
    "            xyz = xyz[:, :3, :]\n",
    "        else:\n",
    "            norm = None\n",
    "        \n",
    "        # Transpose to match expected input format\n",
    "        xyz = xyz.transpose(2, 1)\n",
    "        \n",
    "        # Set Abstraction layers\n",
    "        l1_xyz, l1_points = self.sa1(xyz, norm)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        \n",
    "        # FC layers\n",
    "        x = l3_points.view(B, 1024)\n",
    "        x = self.drop1(torch.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(torch.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def _init_pointnet_plus_plus(num_classes=40):\n",
    "    \"\"\"Create and initialize a PointNet++ classification model\"\"\"\n",
    "    model = PointNet2Classification(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv(nn.Module):\n",
    "    def __init__(self, k, in_channels, out_channels):\n",
    "        super(EdgeConv, self).__init__()\n",
    "        self.k = k\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*2, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        \n",
    "    def knn(self, x, k):\n",
    "        inner = -2 * torch.matmul(x.transpose(2, 1), x)\n",
    "        xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "        distance = -xx - inner - xx.transpose(2, 1)\n",
    "        \n",
    "        idx = distance.topk(k=k, dim=-1)[1]\n",
    "        return idx\n",
    "    \n",
    "    def get_graph_feature(self, x, k=20):\n",
    "        batch_size, num_dims, num_points = x.size()\n",
    "        idx = self.knn(x, k)\n",
    "        idx_base = torch.arange(0, batch_size, device=x.device).view(-1, 1, 1) * num_points\n",
    "        idx = idx + idx_base\n",
    "        idx = idx.view(-1)\n",
    "        \n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        feature = x.view(batch_size * num_points, -1)[idx, :]\n",
    "        feature = feature.view(batch_size, num_points, k, num_dims)\n",
    "        x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "        \n",
    "        feature = torch.cat((feature - x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "        return feature\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.get_graph_feature(x, self.k)\n",
    "        x = self.conv(x)\n",
    "        x = x.max(dim=-1, keepdim=False)[0]\n",
    "        return x\n",
    "\n",
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, num_classes=40, k=20):\n",
    "        super(DGCNN, self).__init__()\n",
    "        self.k = k\n",
    "        \n",
    "        # Edge convolution layers\n",
    "        self.edge_conv1 = EdgeConv(k=k, in_channels=3, out_channels=64)\n",
    "        self.edge_conv2 = EdgeConv(k=k, in_channels=64, out_channels=64)\n",
    "        self.edge_conv3 = EdgeConv(k=k, in_channels=64, out_channels=128)\n",
    "        self.edge_conv4 = EdgeConv(k=k, in_channels=128, out_channels=256)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv1d(512, 1024, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input transform\n",
    "        batch_size = x.size(0)\n",
    "        if x.shape[1] == 3 and len(x.shape) == 3:  # BxCxN format\n",
    "            pass\n",
    "        else:  # Assuming BxNxC format (like PointNet)\n",
    "            x = x.transpose(2, 1)\n",
    "            \n",
    "        # Extract edge features\n",
    "        x1 = self.edge_conv1(x)\n",
    "        x2 = self.edge_conv2(x1)\n",
    "        x3 = self.edge_conv3(x2)\n",
    "        x4 = self.edge_conv4(x3)\n",
    "        \n",
    "        # Concatenate features\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        \n",
    "        # MLP\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # Global max pooling\n",
    "        x = torch.max(x, 2)[0]\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.drop1(torch.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(torch.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def _init_dgcnn(num_classes=40):\n",
    "    \"\"\"Create and initialize a DGCNN classification model\"\"\"\n",
    "    model = DGCNN(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointMLP(nn.Module):\n",
    "    def __init__(self, num_classes=40, points=1024, embed_dim=64):\n",
    "        super(PointMLP, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.points = points\n",
    "        \n",
    "        # Point embedding layers\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(3, embed_dim, 1),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(embed_dim, embed_dim, 1),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Hierarchical feature extraction layers\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(256, 512, 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512, 512, 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle input format (B, N, 3) or (B, 3, N)\n",
    "        batch_size = x.size(0)\n",
    "        if x.shape[1] != 3 and x.shape[2] == 3:\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        # Feature extraction\n",
    "        x = self.embedding(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = torch.max(x, 2)[0]\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def _init_pointmlp(num_classes=40):\n",
    "    \"\"\"Create and initialize a PointMLP classification model\"\"\"\n",
    "    model = PointMLP(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointAttentionNet(nn.Module):\n",
    "    def __init__(self, num_classes=40, num_points=1024, embed_dim=128):\n",
    "        super(PointAttentionNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Point embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(3, embed_dim, 1),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Self-attention modules\n",
    "        self.self_attn1 = PointSelfAttention(embed_dim, 4)\n",
    "        self.self_attn2 = PointSelfAttention(embed_dim, 4)\n",
    "        \n",
    "        # Feature processing after attention\n",
    "        self.feature_conv = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, 256, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(256, 512, 1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle input format (B, N, 3) or (B, 3, N)\n",
    "        batch_size = x.size(0)\n",
    "        if x.shape[1] != 3 and x.shape[2] == 3:\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        # Feature embedding\n",
    "        x = self.embedding(x)  # B x embed_dim x N\n",
    "        \n",
    "        # Apply attention\n",
    "        x = self.self_attn1(x)\n",
    "        x = self.self_attn2(x)\n",
    "        \n",
    "        # Feature processing\n",
    "        x = self.feature_conv(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = torch.max(x, 2)[0]\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PointSelfAttention(nn.Module):\n",
    "    def __init__(self, channels, heads=4):\n",
    "        super(PointSelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.channels = channels\n",
    "        self.head_dim = channels // heads\n",
    "        assert self.head_dim * heads == channels, \"channels must be divisible by heads\"\n",
    "        \n",
    "        self.qkv_conv = nn.Conv1d(channels, channels * 3, 1, bias=False)\n",
    "        self.out_conv = nn.Conv1d(channels, channels, 1)\n",
    "        \n",
    "        self.norm1 = nn.BatchNorm1d(channels)\n",
    "        self.norm2 = nn.BatchNorm1d(channels)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels * 2, 1),\n",
    "            nn.BatchNorm1d(channels * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels * 2, channels, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: B x C x N\n",
    "        batch_size, C, N = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Self-attention\n",
    "        qkv = self.qkv_conv(x)  # B x 3C x N\n",
    "        qkv = qkv.reshape(batch_size, 3, self.heads, self.head_dim, N)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]  # B x H x D x N\n",
    "        \n",
    "        # Compute attention scores\n",
    "        q = q.permute(0, 1, 3, 2)  # B x H x N x D\n",
    "        k = k.permute(0, 1, 2, 3)  # B x H x D x N\n",
    "        attn = torch.matmul(q, k) / (self.head_dim ** 0.5)  # B x H x N x N\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        v = v.permute(0, 1, 3, 2)  # B x H x N x D\n",
    "        x = torch.matmul(attn, v)  # B x H x N x D\n",
    "        x = x.permute(0, 1, 3, 2).reshape(batch_size, C, N)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.out_conv(x)\n",
    "        x = self.norm1(x + residual)\n",
    "        \n",
    "        # Feed forward\n",
    "        residual = x\n",
    "        x = self.ff(x)\n",
    "        x = self.norm2(x + residual)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_custom_attention(num_classes=40):\n",
    "    \"\"\"Create and initialize a custom attention-based model\"\"\"\n",
    "    return PointAttentionNet(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import model architectures\n",
    "# Note: These would typically be imported from external files\n",
    "# For demonstration, we'll include placeholders for imports\n",
    "\n",
    "# from models.pointnet2 import PointNet2Classification\n",
    "# from models.dgcnn import DGCNN\n",
    "# from models.pointmlp import PointMLP\n",
    "# from models.custom_attention import PointAttentionNet\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model_name, pretrained_path, num_classes=40):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initialize selected model\n",
    "        if model_name == \"pointnet++\":\n",
    "            self.model = self._init_pointnet_plus_plus()\n",
    "        elif model_name == \"dgcnn\":\n",
    "            self.model = self._init_dgcnn()\n",
    "        elif model_name == \"pointmlp\":\n",
    "            self.model = self._init_pointmlp()\n",
    "        elif model_name == \"custom\":\n",
    "            self.model = self._init_custom_attention()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "            \n",
    "        # Load pre-trained weights\n",
    "        self.load_pretrained(pretrained_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def _init_pointnet_plus_plus(self):\n",
    "        return PointNet2Classification(num_classes=self.num_classes)\n",
    "        \n",
    "    def _init_dgcnn(self):\n",
    "        # Placeholder for DGCNN initialization\n",
    "        return DGCNN(num_classes=self.num_classes)\n",
    "        \n",
    "    def _init_pointmlp(self):\n",
    "        # Placeholder for PointMLP initialization\n",
    "        return PointMLP(num_classes=self.num_classes)\n",
    "        \n",
    "    def _init_custom_attention(self):\n",
    "        # Placeholder for custom attention model initialization\n",
    "        # return PointAttentionNet(num_classes=self.num_classes)\n",
    "        print(\"Initializing custom attention model\")\n",
    "        return None  # Replace with actual model\n",
    "    \n",
    "    def load_pretrained(self, path):\n",
    "        # Load pre-trained weights if the model exists\n",
    "        if self.model is not None and os.path.exists(path):\n",
    "            print(f\"Loading pre-trained weights for {self.model_name} from {path}\")\n",
    "            # self.model.load_state_dict(torch.load(path))\n",
    "        else:\n",
    "            print(f\"No pre-trained weights found at {path}\")\n",
    "    \n",
    "    def evaluate_adversarial_robustness(self, test_loader, attack_type, epsilon):\n",
    "        \"\"\"\n",
    "        Evaluate model robustness against adversarial attacks\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_loader: DataLoader containing test data\n",
    "        attack_type: str, type of attack (e.g., 'fgsm', 'pgd')\n",
    "        epsilon: float, attack strength\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy: float, model accuracy under adversarial attack\n",
    "        \"\"\"\n",
    "        # Placeholder for adversarial evaluation\n",
    "        print(f\"Evaluating {self.model_name} against {attack_type} with Îµ={epsilon}\")\n",
    "        return 0.0\n",
    "    \n",
    "    def evaluate_corruption_robustness(self, test_loader, corruption_type, severity):\n",
    "        \"\"\"\n",
    "        Evaluate model robustness against environmental corruptions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_loader: DataLoader containing test data\n",
    "        corruption_type: str, type of corruption (e.g., 'gaussian', 'impulse')\n",
    "        severity: int, corruption severity level (usually 1-5)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy: float, model accuracy under corruption\n",
    "        \"\"\"\n",
    "        # Placeholder for corruption evaluation\n",
    "        print(f\"Evaluating {self.model_name} against {corruption_type} corruption at severity {severity}\")\n",
    "        return 0.0\n",
    "\n",
    "# Example usage:\n",
    "# evaluator = ModelEvaluator(\"pointnet++\", \"pretrained/pointnet_modelnet40.pth\")\n",
    "# adv_acc = evaluator.evaluate_adversarial_robustness(test_loader, \"pgd\", 0.1)\n",
    "# corr_acc = evaluator.evaluate_corruption_robustness(test_loader, \"gaussian_noise\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import random\n",
    "from scipy.spatial.transform import Rotation\n",
    "import math\n",
    "\n",
    "class ModelNet40Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', num_points=1024, transform=None, random_rotation=True, class_balance=True):\n",
    "        \"\"\"\n",
    "        ModelNet40 Dataset for 3D point cloud classification\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        root_dir: str, path to the ModelNet40 dataset\n",
    "        split: str, 'train' or 'test'\n",
    "        num_points: int, number of points to sample (1024, 2048, or 4096)\n",
    "        transform: callable, optional transform to be applied on a sample\n",
    "        random_rotation: bool, whether to apply random rotation augmentation\n",
    "        class_balance: bool, whether to use class balancing\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.num_points = num_points\n",
    "        self.transform = transform\n",
    "        self.random_rotation = random_rotation\n",
    "        \n",
    "        # Get all class folders\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        # Get all model files\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls_name, split)\n",
    "            if os.path.exists(cls_path):\n",
    "                model_files = glob.glob(os.path.join(cls_path, '*.off'))\n",
    "                self.files.extend(model_files)\n",
    "                self.labels.extend([self.class_to_idx[cls_name]] * len(model_files))\n",
    "        \n",
    "        # Prepare weights for class balancing\n",
    "        self.weights = None\n",
    "        if class_balance:\n",
    "            # Count instances per class\n",
    "            class_counts = np.zeros(len(self.classes))\n",
    "            for label in self.labels:\n",
    "                class_counts[label] += 1\n",
    "            # Calculate weights inversely proportional to class frequency\n",
    "            self.weights = 1.0 / class_counts\n",
    "            # Assign weight to each sample\n",
    "            self.sample_weights = np.array([self.weights[label] for label in self.labels])\n",
    "            self.sample_weights = torch.from_numpy(self.sample_weights).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load point cloud (placeholder - actual loading depends on file format)\n",
    "        # For OFF files, you'd parse the OFF format\n",
    "        # For this example, we'll generate a random point cloud\n",
    "        # In practice, replace this with actual file loading\n",
    "        points = np.random.rand(1024, 3)  # Placeholder\n",
    "        \n",
    "        # Sample num_points points\n",
    "        if points.shape[0] > self.num_points:\n",
    "            indices = np.random.choice(points.shape[0], self.num_points, replace=False)\n",
    "            points = points[indices]\n",
    "        elif points.shape[0] < self.num_points:\n",
    "            # Upsample by duplicating\n",
    "            indices = np.random.choice(points.shape[0], self.num_points - points.shape[0], replace=True)\n",
    "            points = np.vstack([points, points[indices]])\n",
    "        \n",
    "        # Normalize to unit sphere\n",
    "        center = np.mean(points, axis=0)\n",
    "        points = points - center\n",
    "        dist = np.max(np.sqrt(np.sum(points ** 2, axis=1)))\n",
    "        points = points / dist\n",
    "        \n",
    "        # Apply random rotation\n",
    "        if self.split == 'train' and self.random_rotation:\n",
    "            # Random rotation around z-axis\n",
    "            theta = np.random.uniform(0, 2 * np.pi)\n",
    "            rotation_matrix = np.array([\n",
    "                [np.cos(theta), -np.sin(theta), 0],\n",
    "                [np.sin(theta), np.cos(theta), 0],\n",
    "                [0, 0, 1]\n",
    "            ])\n",
    "            points = points @ rotation_matrix\n",
    "        \n",
    "        if self.transform:\n",
    "            points = self.transform(points)\n",
    "        \n",
    "        return points, label\n",
    "\n",
    "class PointCloudCorruptor:\n",
    "    \"\"\"Class for adding various environmental corruptions to point clouds\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_snow(points, density=0.1, scatter_strength=0.05):\n",
    "        \"\"\"Add snow effect to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        density: float, snow density (0.0-1.0)\n",
    "        scatter_strength: float, scattering effect strength\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        num_points = points.shape[0]\n",
    "        num_snow_points = int(num_points * density)\n",
    "        \n",
    "        # Generate snow points\n",
    "        snow_points = np.random.uniform(-1, 1, size=(num_snow_points, 3))\n",
    "        \n",
    "        # Create scattering effect\n",
    "        scatter = np.random.normal(0, scatter_strength, size=(num_points, 3))\n",
    "        points_scattered = points + scatter\n",
    "        \n",
    "        # Randomly select points to replace with snow\n",
    "        indices = np.random.choice(num_points, num_snow_points, replace=False)\n",
    "        points_scattered[indices] = snow_points\n",
    "        \n",
    "        return points_scattered\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_rain(points, density=0.1, drop_length=0.05):\n",
    "        \"\"\"Add rain effect to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        density: float, rain density (0.0-1.0)\n",
    "        drop_length: float, length of rain drops\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        num_points = points.shape[0]\n",
    "        num_rain_points = int(num_points * density)\n",
    "        \n",
    "        # Generate rain points (starting positions)\n",
    "        rain_points = np.random.uniform(-1, 1, size=(num_rain_points, 3))\n",
    "        # Make rain streaks by extending in mostly downward direction\n",
    "        rain_directions = np.random.normal(0, 0.1, size=(num_rain_points, 2))\n",
    "        rain_directions = np.column_stack([rain_directions, -np.abs(np.random.normal(0, 0.5, num_rain_points))])\n",
    "        rain_directions /= np.linalg.norm(rain_directions, axis=1, keepdims=True)\n",
    "        \n",
    "        # Create rain droplet streaks\n",
    "        rain_streaks = rain_points + rain_directions * drop_length\n",
    "        \n",
    "        # Randomly select points to replace with rain\n",
    "        indices = np.random.choice(num_points, num_rain_points, replace=False)\n",
    "        points_with_rain = points.copy()\n",
    "        points_with_rain[indices] = rain_streaks\n",
    "        \n",
    "        return points_with_rain\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_fog(points, density=0.2):\n",
    "        \"\"\"Add fog effect to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        density: float, fog density (0.0-1.0)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        num_points = points.shape[0]\n",
    "        \n",
    "        # Calculate distance from origin for each point\n",
    "        distances = np.linalg.norm(points, axis=1)\n",
    "        \n",
    "        # Attenuate points based on distance and fog density\n",
    "        attenuation = np.exp(-density * distances)\n",
    "        \n",
    "        # Apply attenuation: closer to 0 means more fog effect\n",
    "        # We'll add random displacement proportional to fog intensity\n",
    "        fog_displacement = (1 - attenuation).reshape(-1, 1) * np.random.normal(0, 0.1, size=(num_points, 3))\n",
    "        points_with_fog = points + fog_displacement\n",
    "        \n",
    "        # Randomly drop some distant points (completely obscured by fog)\n",
    "        dropout_prob = 1 - np.exp(-density * distances * 2)\n",
    "        dropout_mask = np.random.random(num_points) > dropout_prob\n",
    "        \n",
    "        # Create a mix of original and fogged points\n",
    "        return points_with_fog * dropout_mask.reshape(-1, 1) + (1 - dropout_mask.reshape(-1, 1)) * np.random.uniform(-0.1, 0.1, size=(num_points, 3))\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(points, sigma=0.02):\n",
    "        \"\"\"Add Gaussian noise to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        sigma: float, standard deviation relative to object size\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(0, sigma, size=points.shape)\n",
    "        return points + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_depth_noise(points, k=0.05):\n",
    "        \"\"\"Add depth-dependent noise (more noise at greater distances)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        k: float, noise coefficient\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        # Calculate distance from origin\n",
    "        distances = np.linalg.norm(points, axis=1)\n",
    "        \n",
    "        # Scale noise by distance (farther = more noise)\n",
    "        noise_scale = k * distances.reshape(-1, 1)\n",
    "        noise = np.random.normal(0, 1, size=points.shape) * noise_scale\n",
    "        \n",
    "        return points + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_occlusion(points, ratio=0.2, mode='random'):\n",
    "        \"\"\"Add occlusion to point cloud\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        points: np.ndarray, shape (N, 3)\n",
    "        ratio: float, percentage of points to occlude (0.0-1.0)\n",
    "        mode: str, 'random', 'region', or 'semantic'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        corrupted_points: np.ndarray, shape (N, 3)\n",
    "        \"\"\"\n",
    "        num_points = points.shape[0]\n",
    "        num_to_occlude = int(num_points * ratio)\n",
    "        \n",
    "        if mode == 'random':\n",
    "            # Random occlusion\n",
    "            mask = np.ones(num_points, dtype=bool)\n",
    "            indices = np.random.choice(num_points, num_to_occlude, replace=False)\n",
    "            mask[indices] = False\n",
    "            return points[mask]\n",
    "        \n",
    "        elif mode == 'region':\n",
    "            # Region-based occlusion\n",
    "            center = np.random.uniform(-0.5, 0.5, size=3)\n",
    "            radius = np.random.uniform(0.2, 0.5)\n",
    "            \n",
    "            # Calculate distances to the center\n",
    "            distances = np.linalg.norm(points - center, axis=1)\n",
    "            \n",
    "            # Keep points outside the sphere\n",
    "            mask = distances > radius\n",
    "            return points[mask]\n",
    "        \n",
    "        elif mode == 'semantic':\n",
    "            # Semantic occlusion (simplified - just occludes one side)\n",
    "            # In real implementation, this would target specific semantic parts\n",
    "            dimension = np.random.randint(0, 3)  # Choose x, y, or z\n",
    "            threshold = np.median(points[:, dimension])\n",
    "            mask = points[:, dimension] > threshold\n",
    "            return points[mask]\n",
    "        \n",
    "        return points\n",
    "\n",
    "def prepare_dataloaders(dataset_path, batch_size=32, num_points=1024, num_workers=4):\n",
    "    \"\"\"\n",
    "    Prepare train and test data loaders\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_path: str, path to ModelNet40 dataset\n",
    "    batch_size: int, batch size\n",
    "    num_points: int, number of points per sample\n",
    "    num_workers: int, number of dataloader workers\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_loader: DataLoader for training data\n",
    "    test_loader: DataLoader for test data\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = ModelNet40Dataset(\n",
    "        root_dir=dataset_path,\n",
    "        split='train',\n",
    "        num_points=num_points,\n",
    "        random_rotation=True,\n",
    "        class_balance=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = ModelNet40Dataset(\n",
    "        root_dir=dataset_path,\n",
    "        split='test',\n",
    "        num_points=num_points,\n",
    "        random_rotation=False,\n",
    "        class_balance=False\n",
    "    )\n",
    "    \n",
    "    # Create weighted sampler for class balancing\n",
    "    if train_dataset.weights is not None:\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=train_dataset.sample_weights,\n",
    "            num_samples=len(train_dataset),\n",
    "            replacement=True\n",
    "        )\n",
    "    else:\n",
    "        sampler = None\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def get_corrupted_dataset(dataset, corruption_type, severity):\n",
    "    \"\"\"\n",
    "    Create a corrupted version of a dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset: Dataset object\n",
    "    corruption_type: str, type of corruption\n",
    "    severity: int or float, severity level\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    corrupted_dataset: Dataset with corruption applied\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataset\n",
    "    corrupted_dataset = copy.deepcopy(dataset)\n",
    "    \n",
    "    # Define corruption parameters based on severity (1-5 scale)\n",
    "    if corruption_type == 'gaussian_noise':\n",
    "        param = {1: 0.01, 2: 0.02, 3: 0.03, 4: 0.04, 5: 0.05}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_gaussian_noise(x, sigma=param)\n",
    "    \n",
    "    elif corruption_type == 'snow':\n",
    "        density = {1: 0.05, 2: 0.1, 3: 0.15, 4: 0.2, 5: 0.3}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_snow(x, density=density)\n",
    "    \n",
    "    elif corruption_type == 'rain':\n",
    "        density = {1: 0.05, 2: 0.1, 3: 0.15, 4: 0.2, 5: 0.3}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_rain(x, density=density)\n",
    "    \n",
    "    elif corruption_type == 'fog':\n",
    "        density = {1: 0.05, 2: 0.1, 3: 0.2, 4: 0.3, 5: 0.4}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_fog(x, density=density)\n",
    "    \n",
    "    elif corruption_type == 'depth_noise':\n",
    "        param = {1: 0.01, 2: 0.02, 3: 0.04, 4: 0.06, 5: 0.1}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_depth_noise(x, k=param)\n",
    "    \n",
    "    elif corruption_type == 'occlusion':\n",
    "        ratio = {1: 0.1, 2: 0.15, 3: 0.2, 4: 0.25, 5: 0.3}[severity]\n",
    "        transform = lambda x: PointCloudCorruptor.add_occlusion(x, ratio=ratio)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported corruption type: {corruption_type}\")\n",
    "    \n",
    "    # Set the transform in the corrupted dataset\n",
    "    corrupted_dataset.transform = transform\n",
    "    \n",
    "    return corrupted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
